{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c299207-1ee5-4370-8303-36494bf9b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import cv2\n",
    "import wandb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"model-b-classifier\", config={\n",
    "    \"img_width\": 128,\n",
    "    \"img_height\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 100,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"VGG16\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080bc51a-32c0-4775-a4cc-5f681d82e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = wandb.config\n",
    "data_dir = r'/opt/notebooks/Applied-AI-Midterm/Data/srgan-images/train'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(torch.cuda.get_device_name(cuda_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb19180-3a2d-4640-a152-fb6e44ba4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset to handle your directory structure\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_list[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        label = 0 if 'cat' in img_path else 1\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ad81c-fd2f-4ba2-b619-383f2ab4a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering all image paths\n",
    "all_images = [os.path.join(data_dir, img) for img in os.listdir(data_dir)]\n",
    "train_size = int(0.8 * len(all_images))\n",
    "valid_size = len(all_images) - train_size\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "train_images, valid_images = torch.utils.data.random_split(all_images, [train_size, valid_size])\n",
    "\n",
    "# Data transformation and augmentation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((config.img_width, config.img_height)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((config.img_width, config.img_height)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Creating datasets and loaders\n",
    "train_dataset = CustomImageDataset(train_images, transform=data_transforms['train'])\n",
    "valid_dataset = CustomImageDataset(valid_images, transform=data_transforms['valid'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2dc549-dac7-4549-ad09-8dc4d810efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images_from_loader(loader, title):\n",
    "    # Fetch a batch of images\n",
    "    images, labels = next(iter(loader))\n",
    "    images = images.numpy().transpose((0, 2, 3, 1))\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.imshow(images[i] * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "        ax.set_title(f'Label: {\"Cat\" if labels[i] == 0 else \"Dog\"}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Display some train images\n",
    "show_images_from_loader(train_loader, \"Sample Scaled Train Images\")\n",
    "\n",
    "# Display some validation images\n",
    "show_images_from_loader(valid_loader, \"Sample Scaled Validation Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c9b46-5242-4220-9bf9-d8094dde08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained VGG16 model\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 2)  # Modify the output layer to match the number of classes\n",
    "\n",
    "# Freeze layers except the last few\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=[0 if 'cat' in img else 1 for img in all_images])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c20402-47ec-4d08-9830-3c47488e35f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Early stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        '''Saves model when validation loss decreases.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.best_val_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path)\n",
    "        self.best_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329dc24-f03e-4323-9567-d57f9e2a57f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, valid_loader, epochs, patience=5, model_path='best_model.pth'):\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    print(\"Starting training loop...\")\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Wrap train_loader with tqdm for progress visualization\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "\n",
    "        # Log training metrics to wandb\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc})\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc.item())\n",
    "\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        # Wrap valid_loader with tqdm for progress visualization\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(valid_loader.dataset)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc.item())\n",
    "    \n",
    "        # Log validation metrics to wandb\n",
    "        wandb.log({\"epoch\": epoch, \"val_loss\": val_loss, \"val_accuracy\": val_acc})\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss, model, model_path)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    # Return collected metrics\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f4cde0-b30b-4eeb-a176-32baeb05f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, criterion, optimizer, train_loader, valid_loader, epochs)\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6acdef8-2925-413e-ae14-1f74b06dd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_and_save_figures(train_losses, val_losses, train_accuracies, val_accuracies, save_dir='/opt/notebooks/Applied-AI-Midterm/figs'):\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'loss_plot.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'accuracy_plot.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with collected metrics\n",
    "plot_and_save_figures(train_losses, val_losses, train_accuracies, val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590b666-97b8-4f6e-8fb3-3af074a200e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfc6b6-9c21-45a0-970d-8decfe151322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (applied-ai)",
   "language": "python",
   "name": "applied-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
