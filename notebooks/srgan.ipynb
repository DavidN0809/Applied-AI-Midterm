{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plotter_lib\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(\n",
    "    project='srgan-training', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(torch.cuda.get_device_name(cuda_id))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data transforms to be applied to the images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turbojpeg import TurboJPEG\n",
    "\n",
    "# Specify the path to the turbojpeg library\n",
    "lib_path = r'/opt/conda/envs/CC-Chatbot/lib/libturbojpeg.so'\n",
    "jpeg = TurboJPEG(lib_path)\n",
    "\n",
    "# Define the dataset class\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load image using TurboJPEG\n",
    "        image_path = self.image_paths[index]\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            image_data = image_file.read()\n",
    "            image = jpeg.decode(image_data)\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Resize the high-resolution image to low-resolution\n",
    "        image_low = transforms.Resize((32, 32))(image)\n",
    "\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return image_low, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the main directory for the dataset\n",
    "main_dir = r'./Data/train'\n",
    "print(\"Initializing DataLoader...\")\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "for image_name in os.listdir(main_dir):\n",
    "    if image_name.startswith(\"cat\"):\n",
    "        label = 0  # Label for cat\n",
    "    elif image_name.startswith(\"dog\"):\n",
    "        label = 1  # Label for dog\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(main_dir, image_name)\n",
    "    image_paths.append(image_path)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing (70/30 split)\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(image_paths, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create dataset instances for train and test\n",
    "image_datasets = {\n",
    "    'train': CatsDogsDataset(train_paths, train_labels, transform=train_transform),\n",
    "    'test': CatsDogsDataset(test_paths, test_labels, transform=test_transform)\n",
    "}\n",
    "\n",
    "# Create DataLoaders for the datasets\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=0, pin_memory=False),\n",
    "    'test': DataLoader(image_datasets['test'], batch_size=32, shuffle=False, num_workers=0, pin_memory=False)\n",
    "}\n",
    "\n",
    "image_low, image_high, labels = next(iter(dataloaders['train']))\n",
    "\n",
    "# Print the number of images in the train and test sets\n",
    "print(f\"Number of images in train set: {len(image_datasets['train'])}\")\n",
    "print(f\"Number of images in test set: {len(image_datasets['test'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_low.shape)\n",
    "print(image_high.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display an image from the train set\n",
    "plt.imshow(image_low[0].permute(1,2,0))\n",
    "plt.title('Train Set - Low Resolution Image')\n",
    "plt.show()\n",
    "\n",
    "# Display an image from the test set\n",
    "plt.imshow(image_high[0].permute(1,2,0))\n",
    "plt.title('Test Set - High Resolution Image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to visualize a low-resolution image, a high-resolution image, and a generated image.\n",
    "def visualize(lr_image, hr_image, generated_image, label, epoch, pretrain, show=False):\n",
    "    # Move the tensors to the CPU and detach the generated_image tensor.\n",
    "    lr_image = lr_image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "    hr_image = hr_image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "    generated_image = generated_image.cpu().detach().permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Normalize the pixel values to the range [0, 1] for visualization.\n",
    "    lr_image = (lr_image - lr_image.min()) / (lr_image.max() - lr_image.min())\n",
    "    hr_image = (hr_image - hr_image.min()) / (hr_image.max() - hr_image.min())\n",
    "    generated_image = (generated_image - generated_image.min()) / (generated_image.max() - generated_image.min())\n",
    "\n",
    "    # Plot the images.\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n",
    "    axes[0].imshow(lr_image)\n",
    "    axes[0].set_title(\"Low Resolution\")\n",
    "    axes[1].imshow(hr_image)\n",
    "    axes[1].set_title(\"High Resolution\")\n",
    "    axes[2].imshow(generated_image)\n",
    "    axes[2].set_title(\"Generated Image\")\n",
    "\n",
    "    # Add some padding to the top of the figure.\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "\n",
    "    # Add a common title to the figure depending on the pretrain flag.\n",
    "    if pretrain:\n",
    "        fig.suptitle(f\" Epoch {epoch} - {label}\")\n",
    "    else:\n",
    "        fig.suptitle(f\"Epoch {epoch} - {label}\")\n",
    "    \n",
    "    # Create the 'output' directory if it doesn't exist.\n",
    "    if not os.path.exists('./output'):\n",
    "        os.makedirs('./output')\n",
    "    \n",
    "    # Save and show the figure every 5 epochs or if the show flag is set to True.\n",
    "    if epoch % 5 == 0 or show:\n",
    "        fig.savefig(f\"./output/epoch_{epoch}_{label}.png\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual(nn.Module):\n",
    "    def __init__(self, num_res_blocks=16):\n",
    "        super().__init__()\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        \n",
    "        # First layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.LazyConv2d(3, 64, kernel_size=9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # Add Residual blocks\n",
    "        res_blocks = []\n",
    "        for i in range(num_res_blocks):\n",
    "            res_blocks.append(residual(64))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "        \n",
    "        # Second conv layer post residual blocks\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.LazyConv2d(64, 64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d()\n",
    "        )\n",
    "        \n",
    "        # Upsampling Layers\n",
    "        upsample_layers = []\n",
    "        for out_channels in [32, 16, 4]:\n",
    "            upsample_layers += [\n",
    "                nn.LazyConvTranspose2d(64, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.PReLU()\n",
    "            ]\n",
    "        self.upsample_layers = nn.Sequential(*upsample_layers)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.LazyConv2d(4, 3, kernel_size=9, padding=4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        residual = x\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.conv2(x)\n",
    "        x += residual\n",
    "        x = self.upsample_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=9, stride=1, padding=4),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(64),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, padding=1, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(256, kernel_size=3, padding=1, stride=1),\n",
    "            torch.nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(256, kernel_size=3, padding=1, stride=1),\n",
    "            torch.nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "            \n",
    "            nn.LazyConv2d(3, kernel_size=9, stride=1, padding=4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.generator(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "test_input = torch.rand(1,3,32,32).to(device)\n",
    "print(generator(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the pre-training generator\n",
    "class pretraining_generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pretrain = True\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        # Calculate the content loss between SR and HR images\n",
    "        content_loss = self.mse_loss(sr, hr)\n",
    "        return content_loss\n",
    "    \n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vgg19\n",
    "\n",
    "# Define a class for the generator loss function\n",
    "class generator_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load a VGG19 model pre-trained on ImageNet\n",
    "        self.vgg = vgg19(pretrained=True).features[:35].eval()\n",
    "        # Freeze the parameters of the VGG19 model\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, sr, hr, discriminator_fake_output):\n",
    "        # Calculate the content loss between SR and HR images using the VGG19 model\n",
    "        with torch.no_grad():\n",
    "            hr_vgg = self.vgg(hr)\n",
    "        sr_vgg = self.vgg(sr)\n",
    "        content_loss = self.mse_loss(sr_vgg, hr_vgg)\n",
    "\n",
    "        # Calculate the adversarial loss using the discriminator output\n",
    "        adversarial_loss = F.binary_cross_entropy(discriminator_fake_output, torch.ones_like(discriminator_fake_output))\n",
    "\n",
    "        # Calculate the total generator loss as a weighted sum of the content and adversarial losses\n",
    "        generator_loss = content_loss + 1e-3 * adversarial_loss\n",
    "\n",
    "        return generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=3, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(64, kernel_size=3, stride=2),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(128, kernel_size=3, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(128, kernel_size=3, stride=2),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(256, kernel_size=3, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(256, kernel_size=3, stride=2),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(512, kernel_size=3, stride=1),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.LazyConv2d(512, kernel_size=3, stride=2),\n",
    "            nn.LazyBatchNorm2d(),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "            \n",
    "       \n",
    "\n",
    "        # Add the fully connected part before the final nn.Linear layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LazyLinear(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Pass the output through the discriminator blocks (body)\n",
    "        x = self.body(x)\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pass the output through the fully connected part\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, real_pred, fake_pred):\n",
    "        real_label = torch.ones_like(real_pred)\n",
    "        fake_label = torch.zeros_like(fake_pred)\n",
    "\n",
    "        # Loss for real high-resolution images\n",
    "        real_loss = self.bce_loss(real_pred, real_label)\n",
    "\n",
    "        # Loss for generated high-resolution images\n",
    "        fake_loss = self.bce_loss(fake_pred, fake_label)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        total_loss = (real_loss + fake_loss) * 0.5\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "test_input = torch.rand(1,3,128,128).to(device)\n",
    "print(discriminator(test_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'models' directory if it doesn't exist\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_loader = dataloaders['train']\n",
    "test_loader = dataloaders['test']\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the number of epochs and the batch size\n",
    "num_epochs = 100 # 50 epochs of pretraining + 100 of both = 150 total epochs\n",
    "batch_size = 48\n",
    "lr = 0.0001\n",
    "bar_width=150\n",
    "\n",
    "# Define the loss functions\n",
    "pretrain_generator_loss = pretraining_generator().to(device)\n",
    "generator_loss_fn = generator_loss().to(device)\n",
    "discriminator_loss_fn = discriminator_loss().to(device)\n",
    "\n",
    "# Move the model parameters to the device\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Define the optimizers\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Load the data\n",
    "train_loader = dataloaders['train']\n",
    "test_loader = dataloaders['test']\n",
    "\n",
    "pretrain_loss = []\n",
    "generator_loss_history = []\n",
    "discriminator_loss_history = []\n",
    "\n",
    "# Watch the model with WandB\n",
    "wandb.watch(generator, log=\"all\")\n",
    "wandb.watch(discriminator, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training phase\n",
    "print(\"Starting pre-training phase...\")\n",
    "for epoch in range(50):\n",
    "    generator.train()\n",
    "    pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/50\", ncols=bar_width)\n",
    "    \n",
    "    for i, (lr_images, hr_images, _) in enumerate(train_loader):\n",
    "        # Move the data to the device\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "\n",
    "        # Generate the super-resolved images\n",
    "        sr_images = generator(lr_images)\n",
    "\n",
    "        # Compute the loss and update the parameters\n",
    "        generator_loss_value = pretrain_generator_loss(sr_images, hr_images)\n",
    "        generator_optimizer.zero_grad()\n",
    "        generator_loss_value.backward(retain_graph=True)\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        pretrain_loss.append(generator_loss_value.item())\n",
    "        \n",
    "        # Log loss to WandB\n",
    "        wandb.log({\"Pretrain Loss\": generator_loss_value.item()})\n",
    "\n",
    "        # Print the loss value every 10 batches\n",
    "        if (i+1) % 10 == 0:\n",
    "            pbar.set_postfix({\"Gen loss\": f\"{generator_loss_value.item():.4f}\"})\n",
    "        pbar.update()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save the generated images for visualization\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        lr_images, hr_images, _ = next(iter(test_loader))\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "        sr_images = generator(lr_images)\n",
    "        visualize(lr_images[0], hr_images[0], sr_images[0], \"Pre-Training\", epoch+1, True)\n",
    "        # Log the generated image to WandB\n",
    "        wandb.log({\"Generated Image\": [wandb.Image(sr_images[0].cpu(), caption=f\"Epoch {epoch+1}\")]})\n",
    "    # Save the generator model every 5 epochs\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        torch.save(generator.state_dict(), f\"./models/pretrain_generator_epoch_{epoch+1}.pt\")\n",
    "        wandb.save(f\"./models/pretrain_generator_epoch_{epoch+1}.pt\")\n",
    "        \n",
    "# Adversarial training phase\n",
    "print(\"Starting adversarial training phase...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=bar_width)\n",
    "    for i, (lr_images, hr_images, _) in enumerate(train_loader):\n",
    "        # Move the data to the device\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "\n",
    "        # Generate the super-resolved images\n",
    "        sr_images = generator(lr_images)\n",
    "\n",
    "        # Train the discriminator\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_labels = torch.ones(hr_images.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(hr_images.size(0), 1).to(device)\n",
    "        discriminator_real_output = discriminator(hr_images)\n",
    "        discriminator_fake_output = discriminator(sr_images)\n",
    "\n",
    "        discriminator_real_loss = discriminator_loss_fn(discriminator_real_output, real_labels)\n",
    "        discriminator_fake_loss = discriminator_loss_fn(discriminator_fake_output, fake_labels)\n",
    "        discriminator_loss_value = discriminator_real_loss + discriminator_fake_loss\n",
    "        discriminator_loss_value.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        generator_optimizer.zero_grad()\n",
    "        sr_images = generator(lr_images)\n",
    "        discriminator_fake_output = discriminator(sr_images)\n",
    "        generator_loss_value = generator_loss_fn(sr_images, hr_images, discriminator_fake_output)\n",
    "        generator_loss_value.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        discriminator_loss_history.append(discriminator_loss_value.item())\n",
    "        generator_loss_history.append(generator_loss_value.item())\n",
    "\n",
    "        # Log losses to WandB\n",
    "        wandb.log({\n",
    "            \"Generator Loss\": generator_loss_value.item(),\n",
    "            \"Discriminator Loss\": discriminator_loss_value.item()\n",
    "        })\n",
    "\n",
    "        # Print the loss value every 10 batches\n",
    "        if (i+1) % 10 == 0:\n",
    "            pbar.set_postfix({\"Gen loss\": f\"{generator_loss_value.item():.4f}\", \"Discr loss\": f\"{discriminator_loss_value.item():.4f}\"})\n",
    "        pbar.update()\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save the generator and discriminator models every 5 epochs\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        torch.save(generator.state_dict(), f\"./models/generator_epoch_{epoch+1}.pt\")\n",
    "        torch.save(discriminator.state_dict(), f\"./models/discriminator_epoch_{epoch+1}.pt\")\n",
    "        wandb.save(f\"./models/generator_epoch_{epoch+1}.pt\")\n",
    "        wandb.save(f\"./models/discriminator_epoch_{epoch+1}.pt\")\n",
    "\n",
    "    # Save the generated images for visualization\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        lr_images, hr_images, _ = next(iter(test_loader))\n",
    "        lr_images = lr_images.to(device)\n",
    "        hr_images = hr_images.to(device)\n",
    "        sr_images = generator(lr_images)\n",
    "        visualize(lr_images[0], hr_images[0], sr_images[0], \"Adversarial Training\", epoch+1, True)\n",
    "        # Log the generated image to WandB\n",
    "        wandb.log({\"Generated Image\": [wandb.Image(sr_images[0].cpu(), caption=f\"Epoch {epoch+1}\")]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename='./output/epoch_100_Adversarial Training.png')\n",
    "#last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CC-Chatbot)",
   "language": "python",
   "name": "cc-chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
